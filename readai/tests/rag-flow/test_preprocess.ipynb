{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.core.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    get_leaf_nodes,\n",
    "    get_root_nodes,\n",
    ")\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from readai.components.custom.text_splitters.chinese_text_splitter import (\n",
    "    ChineseRecursiveTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用epubloader，效果不太理想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：使用 UnstructuredEPubLoader 加载 EPUB 文档\n",
    "test_data_path = Path(\"/Users/pegasus/media/library\")\n",
    "book_name = \"跃迁：成为高手的技术.epub\"\n",
    "book_path = test_data_path / book_name\n",
    "loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "documents = loader.load()\n",
    "# 查看节点数量\n",
    "print(f\"节点数量: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改成markdown文件后，相对来说可操作空间更大了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点数量: 41\n",
      "{'source': '/Users/pegasus/workplace/mygits/readest-ai/readai-backend/readai/tests/data/comunication_cleaned.md', 'languages': ['zho'], 'file_directory': '/Users/pegasus/workplace/mygits/readest-ai/readai-backend/readai/tests/data', 'filename': 'comunication_cleaned.md', 'filetype': 'text/markdown', 'last_modified': '2025-04-01T20:55:25', 'category': 'NarrativeText', 'element_id': '6ce09934c1462a9ccfa47a5a073b3d06'}\n"
     ]
    }
   ],
   "source": [
    "test_data_path = Path(\n",
    "    \"/Users/pegasus/workplace/mygits/readest-ai/readai-backend/readai/tests/data\"\n",
    ")\n",
    "book_name = \"comunication_cleaned.md\"\n",
    "book_path = test_data_path / book_name\n",
    "loader = UnstructuredMarkdownLoader(book_path, mode=\"elements\")\n",
    "documents = loader.load()\n",
    "# 查看节点数量\n",
    "print(f\"节点数量: {len(documents)}\")\n",
    "print(documents[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NarrativeText', 'UncategorizedText', 'Title'}\n",
      "最长内容长度: 13639\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "nodes_list = []\n",
    "node_category = set()\n",
    "# 检查所有节点的metadata中一共出现了哪几种category\n",
    "# 记录过程中最长的内容长度\n",
    "max_text_len = 0\n",
    "for doc in documents:\n",
    "    node_category.add(doc.metadata[\"category\"])\n",
    "    node_data = {\n",
    "        \"text\": doc.page_content,\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"text_len\": len(doc.page_content),\n",
    "    }\n",
    "    nodes_list.append(node_data)\n",
    "    if len(doc.page_content) > max_text_len:\n",
    "        max_text_len = len(doc.page_content)\n",
    "print(node_category)\n",
    "print(f\"最长内容长度: {max_text_len}\")\n",
    "\n",
    "# 保存到 json 文件\n",
    "# with open(\"非暴力沟通md.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(nodes_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，加载后的段落更完整，更符合直觉的按照章节内容分割成多个节点了，这样可以进一步操作，构建多层级节点，同时也可以去生成一些更好的总结性的文本作为节点内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d0a6dde4c744419e45d0d9c3d84ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from readai.components.custom.text_splitters.sentence import SentenceSplitter\n",
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "ingest_docs = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata) for doc in documents\n",
    "]\n",
    "# 创建SentenceSplitter实例\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512,  # 每个块的最大token数\n",
    "    chunk_overlap=50,  # 块之间的重叠token数\n",
    "    include_metadata=True,  # 包含元数据\n",
    "    include_prev_next_rel=True,  # 维护块之间的前后关系\n",
    ")\n",
    "\n",
    "# 直接调用spltter的方式\n",
    "processed_nodes = sentence_splitter.get_nodes_from_documents(\n",
    "    ingest_docs, show_progress=True\n",
    ")\n",
    "\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=[\n",
    "#         sentence_splitter,\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# processed_nodes = pipeline.run(\n",
    "#     documents=ingest_docs,\n",
    "#     show_progress=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文长度: 1526, 分割后片段数: 3\n",
      "马歇尔·卢森堡博士发现了一种沟通方式，依照它来谈话和聆听，能使人们情意相通，和谐相处，这就是“非暴力沟通”。 做为一个遵纪守法的好人，也许我们从来没有把谈话和“暴力”扯上关系。不过如果稍微留意一下现实生活中的谈话方式，并且用心体会各种谈话方式给我们的不同感受，我们一定会发现，有些话确实伤人！言语上的指责、嘲讽、否定、说教以及任意打断、拒不回应、随意出口的评价和结论给我们带来的情感和精神上的创伤甚至比肉体的伤害更加令人痛苦。这些无心或有意的语言暴力让人与人变得冷漠、隔膜、敌视。 非暴力沟通能够： ● 疗愈内心深处的隐秘伤痛； ● 超越个人心智和情感的局限性； ● 突破那些引发愤怒、沮丧、焦虑等负面情绪的思维方式； ● 用不带伤害的方式化解人际间的冲突； ● 学会建立和谐的生命体验。 图书在版编目(CIP)数据 非暴力沟通／（美）马歇尔·卢森堡（Marshall B.Rosenberg）著；刘轶译.-2版（修订本）-北京：华夏出版社有限公司，2021.5 书名原文：Nonviolent Communication ISBN 978-7-5222-0051-4 Ⅰ.①非⋯Ⅱ.①马⋯②刘⋯Ⅲ.①心理交往-通俗读物Ⅳ.①C912.11-49 中国版本图书馆CIP数据核字（2021）第006542号 Translated from the book Nonviolent Communication:A Language of Life 3rd Edition,ISBN 13/10:9781892005281/189200528X by Marshall B.Rosenberg. Copyright ? Fall 2015 Puddle Dancer Press,published by Puddle Dancer Press. All rights reserved. Used with permission. For further information about Nonviolent Communication(TM) please visit the Center for Nonviolent Communication on the Web at:www.cnvc.org. 版权所有翻印必究 北京市版权局著作权合同登记号：图字01-2016-2253号 非暴力沟通 著 者 [美]马歇尔·卢森堡 译 者 刘 轶 责任编辑 朱 悦 责任印制 刘 洋 出版发行 华夏出版社 经 销 新华书店 印 刷 三河市少明印务有限公司 装 订 三河市少明印务有限公司 版 次 2021年5月北京第2版 2021年5月北京第1次印刷 开 本 720x1030 1/16开 印 张 16 彩 页 6页 字 数 190千字 定 价 59.80元 华夏出版社有限公司 地址：北京市东直门外香河园北里4号 邮编：100028 网址：www.hxph.com.cn 电话：(010)64663331(转) 若发现本版图书有印装质量问题，请与我社营销中心联系调换． 有关非暴力沟通的更多信息，请联系非暴力沟通中心，地址如下 Center for Nonviolent Communication (CNVC) 9301 Indian School Rd., NE, Suite 204Albuquerque NM 87112-2861 USA Ph: 505-244-4041 US Only: 800-255-7696 Fax: 505-247-0414 Email: cnvc@CNVC.org Website: www.CNVC.org 目录\n",
      "----------------------------------------------------------------------------------------------------\n",
      "马歇尔·卢森堡博士发现了一种沟通方式，依照它来谈话和聆听，能使人们情意相通，和谐相处，这就是“非暴力沟通”。 做为一个遵纪守法的好人，也许我们从来没有把谈话和“暴力”扯上关系。不过如果稍微留意一下现实生活中的谈话方式，并且用心体会各种谈话方式给我们的不同感受，我们一定会发现，有些话确实伤人！言语上的指责、嘲讽、否定、说教以及任意打断、拒不回应、随意出口的评价和结论给我们带来的情感和精神上的创伤甚至比肉体的伤害更加令人痛苦。这些无心或有意的语言暴力让人与人变得冷漠、隔膜、敌视。 非暴力沟通能够： ● 疗愈内心深处的隐秘伤痛； ● 超越个人心智和情感的局限性； ● 突破那些引发愤怒、沮丧、焦虑等负面情绪的思维方式； ● 用不带伤害的方式化解人际间的冲突； ● 学会建立和谐的生命体验。 图书在版编目(CIP)数据 非暴力沟通／（美）马歇尔·卢森堡（Marshall B.Rosenberg）著；刘轶译.-2版（修订本）-北京：华夏出版社有限公司，2021.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenberg）著；刘轶译.-2版（修订本）-北京：华夏出版社有限公司，2021.5 书名原文：Nonviolent Communication ISBN 978-7-5222-0051-4 Ⅰ.①非⋯Ⅱ.①马⋯②刘⋯Ⅲ.①心理交往-通俗读物Ⅳ.①C912.11-49 中国版本图书馆CIP数据核字（2021）第006542号 Translated from the book Nonviolent Communication:A Language of Life 3rd Edition,ISBN 13/10:9781892005281/189200528X by Marshall B.Rosenberg. Copyright ? Fall 2015 Puddle Dancer Press,published by Puddle Dancer Press. All rights reserved. Used with permission. For further information about Nonviolent Communication(TM) please visit the Center for Nonviolent Communication on the Web at:www.cnvc.org.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All rights reserved. Used with permission. For further information about Nonviolent Communication(TM) please visit the Center for Nonviolent Communication on the Web at:www.cnvc.org. 版权所有翻印必究 北京市版权局著作权合同登记号：图字01-2016-2253号 非暴力沟通 著 者 [美]马歇尔·卢森堡 译 者 刘 轶 责任编辑 朱 悦 责任印制 刘 洋 出版发行 华夏出版社 经 销 新华书店 印 刷 三河市少明印务有限公司 装 订 三河市少明印务有限公司 版 次 2021年5月北京第2版 2021年5月北京第1次印刷 开 本 720x1030 1/16开 印 张 16 彩 页 6页 字 数 190千字 定 价 59.80元 华夏出版社有限公司 地址：北京市东直门外香河园北里4号 邮编：100028 网址：www.hxph.com.cn 电话：(010)64663331(转) 若发现本版图书有印装质量问题，请与我社营销中心联系调换． 有关非暴力沟通的更多信息，请联系非暴力沟通中心，地址如下 Center for Nonviolent Communication (CNVC) 9301 Indian School Rd., NE, Suite 204Albuquerque NM 87112-2861 USA Ph: 505-244-4041 US Only: 800-255-7696 Fax: 505-247-0414 Email: cnvc@CNVC.org Website: www.CNVC.org 目录\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 手动调用SentenceSplitter来查看其行为\n",
    "ingest_docs = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata) for doc in documents\n",
    "]\n",
    "# 对一个doc进行分割后查看细节\n",
    "test_text = ingest_docs[0].text\n",
    "test_splits = sentence_splitter.split_text(test_text)\n",
    "print(f\"原文长度: {len(test_text)}, 分割后片段数: {len(test_splits)}\")\n",
    "# 检查原文以及分割后的结果\n",
    "print(test_text)\n",
    "print(\"-\" * 100)\n",
    "for text in test_splits:\n",
    "    print(text)\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    }
   ],
   "source": [
    "# 查看node有哪些属性,按照json格式输出\n",
    "print(len(processed_nodes))\n",
    "# processed_nodes节点输出全部看看,保存内容到json文件中\n",
    "with open(\"processed_nodes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for node in processed_nodes:\n",
    "        # 数据转json\n",
    "        json_data = json.dumps(node.to_dict(), ensure_ascii=False, indent=2)\n",
    "        f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用分级摘要节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e49b94208cd4733ad84fe19ccfd827c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (123) is close to chunk size (128). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (123) is close to chunk size (128). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (123) is close to chunk size (128). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (123) is close to chunk size (128). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Metadata length (128) is longer than chunk size (128). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m\n\u001b[1;32m     10\u001b[0m hierarchical_parser \u001b[38;5;241m=\u001b[39m HierarchicalNodeParser\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m     11\u001b[0m     include_prev_next_rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, chunk_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 为章节级别节点创建摘要\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# summary_extractor = SummaryExtractor(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     summaries=[\"chapter_summary\"],  # 摘要类型\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 创建处理管道\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m book_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mingest_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m leaf_nodes \u001b[38;5;241m=\u001b[39m get_leaf_nodes(book_nodes)\n\u001b[1;32m     24\u001b[0m root_nodes \u001b[38;5;241m=\u001b[39m get_root_nodes(book_nodes)\n",
      "File \u001b[0;32m~/workplace/mygits/readest-ai/readai-backend/readai/components/custom/text_splitters/hierarchical.py:227\u001b[0m, in \u001b[0;36mHierarchicalNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: a bit of a hack rn for tqdm\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents_with_progress:\n\u001b[0;32m--> 227\u001b[0m     nodes_from_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursively_get_nodes_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     all_nodes\u001b[38;5;241m.\u001b[39mextend(nodes_from_doc)\n\u001b[1;32m    230\u001b[0m event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: all_nodes})\n",
      "File \u001b[0;32m~/workplace/mygits/readest-ai/readai-backend/readai/components/custom/text_splitters/hierarchical.py:200\u001b[0m, in \u001b[0;36mHierarchicalNodeParser._recursively_get_nodes_from_nodes\u001b[0;34m(self, nodes, level, show_progress)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# now for each sub-node, recursively split into sub-sub-nodes, and add\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_parser_ids) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     sub_sub_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursively_get_nodes_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     sub_sub_nodes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/workplace/mygits/readest-ai/readai-backend/readai/components/custom/text_splitters/hierarchical.py:200\u001b[0m, in \u001b[0;36mHierarchicalNodeParser._recursively_get_nodes_from_nodes\u001b[0;34m(self, nodes, level, show_progress)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# now for each sub-node, recursively split into sub-sub-nodes, and add\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_parser_ids) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     sub_sub_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursively_get_nodes_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     sub_sub_nodes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/workplace/mygits/readest-ai/readai-backend/readai/components/custom/text_splitters/hierarchical.py:184\u001b[0m, in \u001b[0;36mHierarchicalNodeParser._recursively_get_nodes_from_nodes\u001b[0;34m(self, nodes, level, show_progress)\u001b[0m\n\u001b[1;32m    180\u001b[0m sub_nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes_with_progress:\n\u001b[1;32m    182\u001b[0m     cur_sub_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_parser_map\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_parser_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m--> 184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# add parent relationship from sub node to parent node\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# add child relationship from parent node to sub node\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# NOTE: Only add relationships if level > 0, since we don't want to add\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# relationships for the top-level document objects that we are splitting\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/readai-vv5wOT8E-py3.10/lib/python3.10/site-packages/llama_index/core/node_parser/interface.py:165\u001b[0m, in \u001b[0;36mNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m doc_id_to_document \u001b[38;5;241m=\u001b[39m {doc\u001b[38;5;241m.\u001b[39mid_: doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents}\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    163\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mNODE_PARSING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mDOCUMENTS: documents}\n\u001b[1;32m    164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 165\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_parsed_nodes(nodes, doc_id_to_document)\n\u001b[1;32m    168\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end({EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes})\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/readai-vv5wOT8E-py3.10/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    325\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/readai-vv5wOT8E-py3.10/lib/python3.10/site-packages/llama_index/core/node_parser/interface.py:260\u001b[0m, in \u001b[0;36mMetadataAwareTextSplitter._parse_nodes\u001b[0;34m(self, nodes, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes_with_progress:\n\u001b[1;32m    259\u001b[0m     metadata_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_str(node)\n\u001b[0;32m--> 260\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text_metadata_aware\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     all_nodes\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    265\u001b[0m         build_nodes_from_splits(splits, node, id_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_func)\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/readai-vv5wOT8E-py3.10/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    325\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/workplace/mygits/readest-ai/readai-backend/readai/components/custom/text_splitters/sentence.py:159\u001b[0m, in \u001b[0;36mSentenceSplitter.split_text_metadata_aware\u001b[0;34m(self, text, metadata_str)\u001b[0m\n\u001b[1;32m    157\u001b[0m effective_chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size \u001b[38;5;241m-\u001b[39m metadata_len\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_chunk_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is longer than chunk size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Consider increasing the chunk size or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecreasing the size of your metadata to avoid this.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m effective_chunk_size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is close to chunk size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Resulting chunks are less than 50 tokens. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    171\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata length (128) is longer than chunk size (128). Consider increasing the chunk size or decreasing the size of your metadata to avoid this."
     ]
    }
   ],
   "source": [
    "# 1. 预处理阶段\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from readai.components.custom.text_splitters.hierarchical import HierarchicalNodeParser\n",
    "from llama_index.core.extractors import SummaryExtractor\n",
    "\n",
    "ingest_docs = [\n",
    "    Document(text=doc.page_content, metadata=doc.metadata) for doc in documents\n",
    "]\n",
    "# 创建层次解析器\n",
    "hierarchical_parser = HierarchicalNodeParser.from_defaults(\n",
    "    include_metadata=False, include_prev_next_rel=True, chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "# 为章节级别节点创建摘要\n",
    "# summary_extractor = SummaryExtractor(\n",
    "#     summaries=[\"chapter_summary\"],  # 摘要类型\n",
    "#     node_types=[\"chapter\"]  # 只为章节节点创建摘要\n",
    "# )\n",
    "\n",
    "# 创建处理管道\n",
    "book_nodes = hierarchical_parser.get_nodes_from_documents(\n",
    "    ingest_docs, show_progress=True\n",
    ")\n",
    "leaf_nodes = get_leaf_nodes(book_nodes)\n",
    "root_nodes = get_root_nodes(book_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1859\n",
      "1436\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(len(book_nodes))\n",
    "print(len(leaf_nodes))\n",
    "print(len(root_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseComponent.to_json of TextNode(id_='160caeec-c20d-4118-847f-c7b265d36f1b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d69b0408-c955-4655-9b84-877d567b5a44', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': '/Users/pegasus/workplace/mygits/readest-ai/readai-backend/readai/tests/data/comunication_cleaned.md', 'languages': ['zho'], 'file_directory': '/Users/pegasus/workplace/mygits/readest-ai/readai-backend/readai/tests/data', 'filename': 'comunication_cleaned.md', 'filetype': 'text/markdown', 'last_modified': '2025-04-01T20:55:25', 'category': 'NarrativeText', 'element_id': '6ce09934c1462a9ccfa47a5a073b3d06'}, hash='e9c1d6555633db3366004ce21db95f26fa7df8c1297658edb5e255f3e1662fc7'), <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='124a2580-b3d2-462f-839a-9a0fbec1c1c6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6aa7743b3b7f8ba0fd02257fdc6146442bb68e3f23947c0e34fe694cce05c055'), RelatedNodeInfo(node_id='a287ecd3-556d-4107-b88f-b04946474163', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a85ce93d60be88395e203e4eee0156f4ec6c34cba0f6efa92745c68a57cacb8f'), RelatedNodeInfo(node_id='1d7ae9f4-e869-4725-97eb-f1c37c99d60c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='56c578b34b57cde67d96ead3cdfd0c32e2b8c732cd792b32e26287c7c64caf75')]}, metadata_template='{key}: {value}', metadata_separator='\\n', text='马歇尔·卢森堡博士发现了一种沟通方式，依照它来谈话和聆听，能使人们情意相通，和谐相处，这就是“非暴力沟通”。 做为一个遵纪守法的好人，也许我们从来没有把谈话和“暴力”扯上关系。不过如果稍微留意一下现实生活中的谈话方式，并且用心体会各种谈话方式给我们的不同感受，我们一定会发现，有些话确实伤人！言语上的指责、嘲讽、否定、说教以及任意打断、拒不回应、随意出口的评价和结论给我们带来的情感和精神上的创伤甚至比肉体的伤害更加令人痛苦。这些无心或有意的语言暴力让人与人变得冷漠、隔膜、敌视。 非暴力沟通能够： ● 疗愈内心深处的隐秘伤痛； ● 超越个人心智和情感的局限性； ● 突破那些引发愤怒、沮丧、焦虑等负面情绪的思维方式； ● 用不带伤害的方式化解人际间的冲突； ● 学会建立和谐的生命体验。 图书在版编目(CIP)数据 非暴力沟通／（美）马歇尔·卢森堡（Marshall B.Rosenberg）著；刘轶译.-2版（修订本）-北京：华夏出版社有限公司，2021.5 书名原文：Nonviolent Communication ISBN 978-7-5222-0051-4 Ⅰ.①非⋯Ⅱ.①马⋯②刘⋯Ⅲ.①心理交往-通俗读物Ⅳ.①C912.11-49 中国版本图书馆CIP数据核字（2021）第006542号 Translated from the book Nonviolent Communication:A Language of Life 3rd Edition,ISBN 13/10:9781892005281/189200528X by Marshall B.Rosenberg. Copyright ? Fall 2015 Puddle Dancer Press,published by Puddle Dancer Press. All rights reserved. Used with permission. For further information about Nonviolent Communication(TM) please visit the Center for Nonviolent Communication on the Web at:www.cnvc.org. 版权所有翻印必究 北京市版权局著作权合同登记号：图字01-2016-2253号 非暴力沟通 著 者 [美]马歇尔·卢森堡 译 者 刘 轶 责任编辑 朱 悦 责任印制 刘 洋 出版发行 华夏出版社 经 销 新华书店 印 刷 三河市少明印务有限公司 装 订 三河市少明印务有限公司 版 次 2021年5月北京第2版 2021年5月北京第1次印刷 开 本 720x1030 1/16开 印 张 16 彩 页 6页 字 数 190千字 定 价 59.80元 华夏出版社有限公司 地址：北京市东直门外香河园北里4号 邮编：100028 网址：www.hxph.com.cn 电话：(010)64663331(转) 若发现本版图书有印装质量问题，请与我社营销中心联系调换． 有关非暴力沟通的更多信息，请联系非暴力沟通中心，地址如下 Center for Nonviolent Communication (CNVC) 9301 Indian School Rd., NE, Suite 204Albuquerque NM 87112-2861 USA Ph: 505-244-4041 US Only: 800-255-7696 Fax: 505-247-0414 Email: cnvc@CNVC.org Website: www.CNVC.org 目录', mimetype='text/plain', start_char_idx=0, end_char_idx=1526, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')>\n"
     ]
    }
   ],
   "source": [
    "# 查看root_nodes\n",
    "print(root_nodes[0].to_json)\n",
    "with open(\"root_nodes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for node in root_nodes:\n",
    "        # 数据转json\n",
    "        json_data = json.dumps(node.to_dict(), ensure_ascii=False, indent=2)\n",
    "        f.write(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 探索TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import (\n",
    "    Document,\n",
    "    MetadataMode,\n",
    "    NodeRelationship,\n",
    "    TextNode,\n",
    "    NodeWithScore,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 检索阶段\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 基础向量检索器\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "# 自动合并检索器\n",
    "auto_retriever = AutoMergingRetriever(\n",
    "    base_retriever=vector_retriever, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 查询引擎\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=auto_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么要转换成markdown\n",
    "因为转换成markdown后，原本epub文件是存在一些结构性内容的，比如标题的层级，相比直接按照epub内容（转html）被读取，转换成markdown后，这种层级内容更加清晰，而且我更好处理\n",
    "- 转换后为什么还要去除一些无效符号？（比如空行）\n",
    "因为目前很多中文分块策略，在区分段落时会根据两个换行符区分，但转markdown后发现有很多多余的空行,清除掉多余的空行其实是可以更方便使用text_spliiter的\n",
    "\n",
    "- 要使用哪一种chunk策略以及node extractor？\n",
    "递归分割？\n",
    "标题提取？\n",
    "句子分割"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readai-vv5wOT8E-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
